version: "2.4"
services:
  chain:
    container_name: chain
    restart: unless-stopped
    build:
      context: ..
      dockerfile: Dockerfile
    volumes:
      - ../src:/service/src
    ports:
      - "33080:8080"
    environment:
      - PYTHONDONTWRITEBYTECODE=1
    # depends_on:
      # - ollama
    command: ["python", "-m", "src.main"]

  # ollama:
  #   # volumes:
  #     # - ./ollama/ollama:/root/.ollama
  #   image: ollama/ollama:latest
  #   runtime: nvidia
  #   container_name: ollama
  #   pull_policy: always
  #   # tty: true
  #   restart: unless-stopped
  #   ports:
  #     - 11434:11434
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    # environment:
    #   - PYTHONDONTWRITEBYTECODE=1
    #   - NVIDIA_VISIBLE_DEVICES=1

  # ollama-webui:
  #   image: ghcr.io/ollama-webui/ollama-webui
  #   container_name: ollama-webui
  #   # volumes:
  #     # - ./ollama/ollama-webui:/app/backend/data
  #   depends_on:
  #     - ollama
  #   ports:
  #     - 3000:8080
  #   environment:
  #     - '/ollama/api=http://ollama:11434/api'
  #   extra_hosts:
  #     - host.docker.internal:host-gateway
  #   restart: unless-stopped
